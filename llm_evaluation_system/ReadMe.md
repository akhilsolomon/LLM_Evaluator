# An LLM Evaluator

This project is a simple and effective evaluation system to analyze the quality of answers generated by a Large Language Model (LLM) used in an internal tool. It helps identify vague or low-quality responses and track improvements over time.

---

##  What It Does

- Automatically scores LLM answers using:
  - **BLEU** (n-gram overlap)
  - **ROUGE** (sequence overlap)
  - **LLM-as-a-Judge** (subjective quality check via GPT)
- Stores and ranks all results
- Helps in building feedback loops and improving the LLM

---

##  Project Structure

```
LLM_Evaluator/
‚îú‚îÄ‚îÄ analysis/             # Main runner script
‚îú‚îÄ‚îÄ data/                 # Input questions and answers
‚îú‚îÄ‚îÄ evaluation/           # Scoring logic (BLEU, ROUGE, LLM Judge)
‚îú‚îÄ‚îÄ results/              # Output scores
‚îú‚îÄ‚îÄ .env                  # OpenAI API key
‚îî‚îÄ‚îÄ README.md
```

---

##  How to Run

1. **Clone the repo**
   ```bash
   git clone https://github.com/akhilsolomon/LLM_Evaluator.git
   cd LLM_Evaluator
   ```

2. **Install requirements**
   ```bash
   pip install -r requirements.txt
   ```

3. **Set your OpenAI key**  
   Create a `.env` file in the root directory:
   ```
   OPENAI_API_KEY=your-key-here
   ```

4. **Run the scorer**
   ```bash
   python3 analysis/scorer.py
   or 
   python analysis/scorer.py
   ```

---

## Where to Check Results

After running the script, scores will be saved in:

```
results/report.json
```

Each entry shows the BLEU, ROUGE, and LLM score for an answer.

---

## ‚ú® Example Input Format

In `data/answers.json`:

```json
{
  "id": "1",
  "question": "How do I access the internal dashboard?",
  "answer": "You can access it at dashboard.company.com using your credentials.",
  "reference": "Use your company email to login at dashboard.company.com"
}
```

## üßë‚Äçüíª Author

Built by **Akhil Solomon** ‚Äî as part of a feedback loop system to improve LLM-based internal tools.

---
